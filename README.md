# Spearman Power Simulation -- Vaccine Aluminum Association

> **Disclaimer:** This code and the statistical methods it implements were generated by AI. A statistician has not yet verified the implementation or the appropriateness of the methods for this application, although it is reassuring that the results of the distinct methods are consistent with each other and that this sort of problem is one AI likely handles well.

> **Important note on data:** Raw data from Karwowski et al. (2018) is not publicly available. All simulations use plausible approximations based on the paper's reported summaries (n, median, IQR, range, and typical vaccination clustering). Results are robust across multiple tie distributions, which provides reasonable confidence despite the lack of exact data.

Statistical power and confidence-interval framework for Spearman rank correlations, designed to evaluate the **non-significant associations** reported in Karwowski et al. (2018) between blood/hair aluminum levels and cumulative vaccine aluminum exposure.

## Background

Karwowski et al. (2018) found no significant Spearman correlations between cumulative vaccine aluminum and either blood aluminum (B-Al) or hair aluminum (H-Al) in a cohort of 9--13-month-old infants. This simulation framework asks: **what correlations *could* the study have detected?**

Because most children followed a similar vaccination schedule, cumulative aluminum values cluster around a narrow range (median 2.9 mg, IQR 0.11 mg), producing **ties in the x-variable**. The framework explicitly models these ties to assess their impact on statistical power.

## Four Cases

Karwowski et al. (2018) recruited 85 infants. Missing B-Al and/or H-Al data and extreme outliers reduce the available sample for each calculation (hence the lower N values in the table below). The paper does not state whether the reported Spearman correlations with vaccine aluminum load were computed with or without outliers. However, when reporting the correlation between B-Al and H-Al, the authors exclude participants with missing data and all extreme outliers, which suggests the vaccine correlations may also have been computed with outliers excluded. To cover both possibilities, the simulation runs four cases: B-Al and H-Al, each with outliers included and excluded.

| Case | Variable | Outliers | N  | Observed rho | P    |
|------|----------|----------|----|-------------|------|
| 1    | B-Al     | Included | 80 | -0.13       | 0.26 |
| 2    | H-Al     | Included | 82 | +0.06       | 0.56 |
| 3    | B-Al     | Excluded | 73 | -0.13       | 0.26 |
| 4    | H-Al     | Excluded | 81 | +0.06       | 0.56 |

## Analysis Goals

We test H₀: ρ = 0 vs H₁: ρ ≠ 0 (two-sided) at α = 0.05. Power is the probability of rejecting H₀ when the true correlation is ρ.

1. **Minimum detectable correlation** -- the smallest |ρ| detectable with **80% power** at **α = 0.05**, for each case × tie structure × distribution shape.
2. **Confidence intervals for observed rho** -- bootstrap and asymptotic CIs. These are the *expected* intervals under the model when the true correlation equals the observed rho, averaged over many simulated datasets, showing the plausible range of true correlations consistent with the data.

## Assumptions

- **Y marginal distribution (aluminum levels):** Y is modeled as log-normal using the reported median and IQR. Because Spearman's rank correlation depends *only* on ranks, the specific continuous distribution of Y has **no effect** on the correlation coefficient, its null distribution, power, or confidence intervals in the non-parametric, linear, or asymptotic methods. The log-normal choice is kept purely for realism (capturing skewness and outliers) and because the Gaussian copula method requires a continuous marginal. Based on the Karwowski (2018) Figure showing the B-Al vs H-Al correlation (N = 71, excluding missing values and outliers), these values have few and mild ties, so this choice for generating y in our simulation is a good approximation.

- **Tie structure distributions:** Three frequency distributions (`even`, `heavy_tail`, `heavy_center`) are tested for sensitivity; differences across them are small, which is reassuring given unavailable raw data. `heavy_center` is considered most plausible for real vaccination schedules: most children followed the schedule and would be clustered in the middle for ages 9–13 months, with the middle 50% under 12 months. More realistic skewed patterns have also been explored but are currently undocumented here (e.g., for k=4 distinct values: only 2–5 observations in the tails, heavy concentration in the second value, and substantial but lower counts in the third). Users can test custom frequencies with `run_single_scenario.py --freq`.

- **Method consistency:** The linear Monte Carlo method produces results that are virtually identical to the recommended non-parametric rank-mixing method. The Gaussian copula method (which uses single-point calibration at rho = 0.30) is retained for comparison and performs very well in all-distinct (no-ties) scenarios. It shows mild attenuation with heavy ties due to jittering but remains a useful secondary check when used with calibration.

## Tie Handling

- X-values are generated with 4--10 distinct values using three frequency distributions: `even`, `heavy_tail`, and `heavy_center`.
- An additional all-distinct baseline (no ties) is computed for each case.
- Total: 84 tied scenarios + 4 all-distinct = 88 scenarios per method.

## Three Analysis Methods

### Non-parametric rank-mixing (recommended)

This is a bivariate simplification of the Iman-Conover method, applied directly in standardized rank space using Cholesky decomposition to induce correlation, followed by reordering of y draws. Like standard Iman-Conover, it preserves the marginal distribution of y exactly via reordering and handles ties in x naturally through midranks. It is the recommended Monte Carlo approach, especially when x has ties. It works by:

1. Computing the standardised midranks of x (handling ties naturally).
2. Generating independent noise ranks via random permutation.
3. Mixing the two at weight rho: `mixed = rho_cal * s_x + sqrt(1 - rho_cal^2) * s_noise`. This is essentially the Cholesky decomposition (Iman-Conover) in the bivariate case applied directly to ranks.
4. Mapping the resulting ordering onto random draws from the target log-normal y-marginal.

**Calibration**: Because ties attenuate the realised Spearman rho relative to the mixing weight, the method includes an automatic calibration step. Two modes are available (configurable via `config.CALIBRATION_MODE` or `--calibration-mode`). In both modes, calibration runs once per unique tie structure and is reused across all rho values during bisection — a major performance improvement over per-rho calibration:
- **Multipoint (default)** (non-parametric rank-mixing only): Probes at rho = 0.10, 0.30, 0.50 and interpolates the calibration curve. Captures nonlinear attenuation (ratio varies with rho) and reduces bias to typically under 0.01. **Accuracy**: Better than single-point when attenuation is nonlinear (e.g. custom or extreme tie structures). **Performance**: ~3× slower calibration on first run per tie structure (~9s vs ~3s) because it probes three values instead of one; cached thereafter and reused for all rho targets.
- **Single-point** (non-parametric rank-mixing and copula): Probes only at rho = 0.30 and applies a constant ratio. Faster (~3× less calibration cost) but can have 0.01–0.03 bias when attenuation is nonlinear. Use for quick exploratory runs.

#### Validity, Assumptions, and Attenuation with Heavy Ties

The method induces an exact Pearson correlation \(\rho_{\mathrm{cal}}\) between the mixed reference ranks and the standardized midranks \(s_x\) via the bivariate Cholesky mixing:

\[
\mathrm{mixed} = \rho_{\mathrm{cal}} \cdot s_x + \sqrt{1 - \rho_{\mathrm{cal}}^2} \cdot s_{\mathrm{noise}}
\]

where:
- \(s_x\) = standardized midranks of x (mean 0, variance 1, incorporating ties via averaging)
- \(s_{\mathrm{noise}}\) = standardized ranks from a random permutation (independent, mean 0, variance 1)

**Derivation of exact Pearson correlation:** Let \(X = s_x\), \(Z = s_{\mathrm{noise}}\) (independent, \(\mathrm{E}[X]=\mathrm{E}[Z]=0\), \(\mathrm{Var}(X)=\mathrm{Var}(Z)=1\)). Then \(B = \rho X + \sqrt{1-\rho^2} Z\):

\[
\mathrm{Var}(B) = \rho^2 \mathrm{Var}(X) + (1-\rho^2) \mathrm{Var}(Z) + 2\rho\sqrt{1-\rho^2}\,\mathrm{Cov}(X,Z) = \rho^2 + (1-\rho^2) = 1
\]

\[
\mathrm{Cov}(X, B) = \rho\,\mathrm{Var}(X) + \sqrt{1-\rho^2}\,\mathrm{Cov}(X,Z) = \rho
\]

\[
\mathrm{Corr}(X, B) = \frac{\mathrm{Cov}(X,B)}{\sqrt{\mathrm{Var}(X)\,\mathrm{Var}(B)}} = \rho
\]

This holds purely from covariance algebra — no assumption is required that \(X\) and \(s_{\mathrm{noise}}\) come from the same distribution, only that they are independent with mean 0 and variance 1.

The final Spearman \(\rho_s\) is the Pearson correlation of the ranks of x and the reordered y. With no ties, this would equal \(\rho_{\mathrm{cal}}\) (ranks are monotone transforms). With heavy ties in x, midranks compress the effective variance/resolution of \(s_x\) (large blocks of identical values), so the mapping from the finely resolved mixed ranks back to the clumped ranks of x attenuates the realised Spearman below \(\rho_{\mathrm{cal}}\).

Approximate attenuation factor: \(\sqrt{1 - \sum(m_j^3 - m_j)/(n^3 - n)}\), where \(m_j\) are tie group sizes (from Fieller-Hartley-Pearson variance correction). The attenuation is nonlinear in \(\rho_{\mathrm{cal}}\), which is why multipoint calibration (probing at 0.10, 0.30, 0.50 and interpolating) is preferred over single-point.

All rank-based simulation methods (including full Iman-Conover) face similar attenuation with heavy ties; the calibration step ensures the realised Spearman matches the target within tolerance (typically < 0.01 bias).

**Why this method?** Unlike the Gaussian copula, it does not rely on the continuous-marginals assumption. Unlike the linear model, it does not assume a parametric relationship between x and y. It handles tied x-values naturally because the mixing operates directly in rank space.

### Gaussian copula

Uses a Gaussian copula with non-parametric marginals and a fitted log-normal for Y. Target Spearman rho is converted to Pearson correlation for sampling via `rho_p = 2 * sin(pi * rho_s / 6)`, then draws from a conditional bivariate normal. Y-values are mapped through the inverse CDF of the fitted log-normal.

**Limitation**: When x has heavy ties, the jittering step that breaks tied ranks collapses rank information and attenuates the realised Spearman rho, leading to underestimated power. Alternatives tested — distributional transform (random uniform within each tie group's CDF band) and adaptive jitter (scaling jitter by tie group size) — improved the mild cases (k=10) but still failed the 0.01 accuracy threshold for heavy ties (k=4). This is a fundamental limitation of the continuous-marginals assumption. The method is retained for comparison purposes and works reliably when there are no ties; with heavy ties, calibration helps compensate for the rank information loss. It applies single-point calibration (probe at rho = 0.30) and is **not** the default method.

### Linear Monte Carlo

Generates `y = a + b*x + noise` on a log scale, calibrated so that the Pearson correlation of ranks approximates the target Spearman rho. This is a parametric model -- useful as a complement to the non-parametric method, but approximate because it assumes a linear log-scale relationship.

### Asymptotic

Closed-form formulas (no simulation). Power uses the non-central t-distribution with df = n-2; CIs use the Fisher z-transform with Bonett-Wright SE = sqrt(1.06/(n-3)). Ties are handled via the Fieller-Hartley-Pearson variance correction throughout (see [Tie correction](#tie-correction-fieller-hartley-pearson) below). Fast and stable, serves as a cross-check against the Monte Carlo methods.

## Installation

```bash
pip install -r requirements.txt
```

Required packages: numpy, scipy, pandas, joblib. **Numba** (recommended) is included in `requirements.txt` for significant speedups. If Numba cannot be installed on your platform, the code falls back to pure NumPy automatically.

### Numba warm-up (optional, recommended)

On the first run after installation, Numba compiles JIT functions (5-15 seconds). To pre-warm the cache:

```bash
python warm_up_numba.py
```

To copy the cache to a cloud VM for zero compile delay:

```bash
rsync -avz ~/.cache/numba/ user@YOUR-IP:~/.cache/numba/
```

## Usage

### Programmatic usage (no command line)

All scripts expose a `main()` function that can be called from Python:

```python
# Full simulation
from run_simulation import main as run_sim
power_df, ci_df, all_distinct_df = run_sim(n_sims=500, skip_linear=True)

# Single scenario
from run_single_scenario import main as run_single
result = run_single(case=3, n_distinct=4, dist_type="heavy_center", n_sims=500)
result = run_single(case=3, freq=[19, 18, 18, 18], power_only=True, verbose=False)

# Accuracy testing
from test_simulation_accuracy import main as test_accuracy
df = test_accuracy(n_sims=50, cases=[3], custom_freq=[(3, [19, 18, 18, 18])])
```

### Full simulation (all methods, all scenarios)

```bash
python run_simulation.py
```

### Quick test run

```bash
python run_simulation.py --n-sims 500 --seed 42
```

### Disable Numba (force pure NumPy fallback)

```bash
python run_simulation.py --n-sims 500 --seed 42 --no-numba
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type heavy_center --n-sims 500 --no-numba
```

### Skip specific methods

```bash
python run_simulation.py --skip-linear --skip-copula   # nonparametric + asymptotic only
python run_simulation.py --skip-nonparametric           # copula + linear + asymptotic
```

### Calibration mode (nonparametric only)

```bash
python run_simulation.py --calibration-mode multipoint   # default: more accurate, ~3× calibration cost
python run_simulation.py --calibration-mode single       # faster calibration for exploratory runs
```

### Filter scenarios

```bash
python run_simulation.py --cases 1,3 --n-distinct 4,10 --dist-types even,heavy_center
```

### Single-scenario quick script

Run power and/or CI for a specific (case, k, distribution) combination:

```bash
# Power + CI for Case 3, k=4, heavy_center
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type heavy_center --n-sims 500

# Power only, skip copula and linear
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type even --power-only --skip-copula --skip-linear

# CI only for all-distinct baseline
python run_single_scenario.py --case 1 --all-distinct --ci-only --n-reps 20 --n-boot 500

# Custom frequency distribution (counts must sum to case's n)
python run_single_scenario.py --case 3 --freq 19,18,18,18 --n-sims 500

# Calibration mode (multipoint default, single for faster runs)
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type heavy_center --n-sims 500 --calibration-mode single
```

### Asymptotic tie-correction modes

```bash
python run_simulation.py --tie-correction both          # report both corrected and uncorrected
python run_simulation.py --tie-correction without_tie_correction
```

## Validation

Test whether generators achieve the target Spearman rho:

```bash
# Quick check on worst-case scenario
python test_simulation_accuracy.py --n-sims 50 --case 3 --n-distinct 4

# Full sweep, all generators
python test_simulation_accuracy.py --n-sims 200

# Specific generators only
python test_simulation_accuracy.py --generators nonparametric,copula --n-sims 100

# Custom frequency distribution (requires --case; counts must sum to case's n)
python test_simulation_accuracy.py --case 3 --freq 19,18,18,18 --n-sims 50

# Save results to CSV
python test_simulation_accuracy.py --n-sims 200 --outfile accuracy_report.csv

# Calibration mode (multipoint default, single for faster runs)
python test_simulation_accuracy.py --n-sims 200 --calibration-mode single
```

The script flags scenarios where |mean_simulated_rho - target_rho| > 0.01 (configurable via `--threshold`). With default n_sims=50–200, some flags may be Monte Carlo noise. To get all scenarios to pass, you may need to increase `--n-sims` to 1000, 5000, or 10000 depending on the tie structure and threshold.

## Output

Results are saved to the `results/` directory:

- `min_detectable_rho.csv` -- minimum detectable rho for all scenarios and methods
- `confidence_intervals.csv` -- bootstrap and asymptotic CIs for observed rho
- `all_distinct_summary.csv` -- combined power + CI table for the 4 all-distinct baselines

## Project Structure

```
config.py                        -- Case parameters, frequency dictionary, settings
data_generator.py                -- X generation (ties) and Y generation (all methods)
power_simulation.py              -- Unified Monte Carlo power (all generators)
power_simulation_copula.py       -- Legacy copula-specific power module
power_simulation_linear.py       -- Legacy linear-specific power module
power_asymptotic.py              -- Asymptotic power and CI formulas
confidence_interval_calculator.py -- Bootstrap CIs (averaged over multiple datasets)
table_outputs.py                 -- Summary table construction and CSV export
run_simulation.py                -- Main orchestrator / CLI entry point
run_single_scenario.py           -- Quick single-scenario testing script
test_simulation_accuracy.py      -- Validation: generator accuracy testing
benchmark_batch_vs_no_batch.py   -- CI: batch vs per-rep path (single scenario)
benchmark_full_grid.py            -- CI: full grid, old vs new path, parallel variants
benchmark_ci_bootstrap.py         -- CI: multipoint vs single calibration
benchmark_power.py                -- Power: vectorize vs scalar, calibration modes
```

## Method Choices and Rationale

### Why non-parametric rank-mixing over copula?

The Gaussian copula assumes continuous marginals for the rank-to-normal-to-rank transformation to preserve Spearman rho. When x has heavy ties (e.g., k=4 with N=73--82), the jittering step that breaks ties is too small to restore the lost rank information, causing systematic attenuation of 0.01--0.06 in the realised rho. Distributional transform and adaptive jitter alternatives were tested but do not fix this for heavy ties. The non-parametric rank-mixing method avoids this by operating directly in rank space and using empirical calibration to compensate for any residual attenuation.

### Why calibration?

With tied x-values, the midrank representation has lower variance than distinct ranks. This means the rank-mixing formula `mixed = rho * s_x + sqrt(1-rho^2) * s_noise` produces a Spearman rho slightly below the target rho. The calibration step computes a rho-independent attenuation ratio by probing at a fixed rho (0.30) and using bisection over 300 samples. This ratio is then multiplied by any target rho to compensate for the attenuation. The ratio is cached per (n, k, dist_type), so the cost is ~3s per unique tie structure.

**If calibration fails for a custom tie structure:** Run `test_simulation_accuracy` on the new structure. If mean realised rho deviates from target by >0.01, try increasing `n_cal` (e.g. 300 → 500 or 1000) or use **multipoint calibration** (default). Multipoint probes at 0.10, 0.30, 0.50 and interpolates, fixing nonlinear attenuation. Use `--calibration-mode single` for faster runs when accuracy is less critical. If even rho_input=0.999 cannot reach the probe, the tie structure has hit a structural ceiling (maximum achievable |rho|) and the method cannot reach that target.

### Why Bonett-Wright SE (1.06 factor)?

The standard Fisher z-transform SE for Pearson r is `sqrt(1/(n-3))`. For Spearman rho, Bonett and Wright (2000) showed that the variance is approximately 6% larger, giving SE = `sqrt(1.06/(n-3))`. The theoretical asymptotic variance factor for Spearman rho (no ties) is π²/9 ≈ 1.0966; Bonett & Wright (2000) recommended the simpler 1.06 approximation (~6% efficiency loss), which is commonly used in practice.

### Why non-central t for power?

The Spearman test statistic `t = rho * sqrt(n-2) / sqrt(1 - rho^2)` follows a non-central t-distribution under the alternative. This is more accurate than the normal approximation (which uses constant H0 variance) because the `sqrt(1 - rho^2)` denominator captures the variance reduction as |rho| grows toward 1.

### Why Fisher z-transform for CIs?

The arctanh transform stabilises the variance of the correlation coefficient and improves normality of the sampling distribution. The back-transform via tanh guarantees the CI stays within [-1, 1] and is properly asymmetric around the point estimate.

### Tie correction (Fieller-Hartley-Pearson)

The Fieller-Hartley-Pearson correction adjusts the **variance of Spearman rho under the null hypothesis (H0)** when ties are present in the rank data. Ties reduce the effective rank information (e.g., many observations sharing the same x-value), so the sampling variance of rho increases. The correction uses the standard formula: define Tx = Σ(tᵢ³ − tᵢ) over tie groups in x (and Ty for y), Dx = (n³−n − Tx)/12, Dy = (n³−n − Ty)/12; then Var(rho) = (1/(n−1)) × (n³−n)²/(144·Dx·Dy). When there are no ties, Dx = Dy = (n³−n)/12 and this reduces to 1/(n−1).

**Where it is used:** (1) **Asymptotic power** — the noncentrality parameter is scaled by √(var_no_ties / var_ties), reducing effective power when ties inflate variance. (2) **Asymptotic CI** — the standard error in z-space is scaled by √(var_ties / var_no_ties), widening the interval appropriately. The impact on SE ranges from negligible (k=10, even: +0.5%) to moderate (k=4, heavy_center: +5.7%).

## Bootstrap CI

The bootstrap uses paired (x,y) resampling with replacement and reports the percentile interval (2.5th and 97.5th percentiles of the bootstrap distribution). The reported CIs average these endpoints over many simulated datasets (n_reps) to estimate the expected bootstrap CI under the model. Two design choices affect correctness and precision:

### Separate RNG streams for data and bootstrap

Data generation and bootstrap resampling use **separate** RNG streams derived from the same seed via `np.random.SeedSequence.spawn(2)`. A shared RNG would advance by `n_boot * n` extra draws per rep inside the bootstrap loop, so the datasets for reps 1..n_reps would depend on `n_boot`. That would make results non-comparable across different `n_boot` values and invalidate the interpretation that "more bootstraps = more accurate." With separate streams, the same seed always produces the same n_reps datasets regardless of `n_boot`.

### n_reps, SE, and reliability of the second decimal

The CI endpoints vary across reps (inter-rep SD ≈ 0.10–0.11 for N=73, slightly lower for N=80+). The SE of the mean endpoint is SD/√n_reps. The 95% CI for the true mean is approximately ±1.96×SE.

| Target | SE | 95% CI half-width | n_reps | When it matters |
|--------|-----|-------------------|--------|-----------------|
| Borderline | 0.005 | ±0.01 | ≈400 | Second decimal can still be off by one unit |
| Strong (README "reliable") | 0.0025 | ±0.005 | ≈1600 | Useful precision; fine when *not* near a rounding boundary |
| Rounding guarantee | 0.00128 | ±0.0025 | ≈7400 | Needed only when the value is *near* a boundary (e.g. 0.345, 0.355) |

**Rounding boundaries:** The boundary between rounding to 0.34 vs 0.35 is 0.345. With n_reps=1600 (95% CI ±0.005), if you observe 0.345 the CI spans [0.34, 0.35] and crosses the boundary—you cannot confidently round. When the value is not near a boundary (e.g. 0.32, 0.38), n_reps=1600 is adequate. The n_reps≈7400 "rounding guarantee" is only needed when you happen to land near a boundary and want confidence in which way to round.

With n_reps=200, SE ≈ 0.007 (worst case N=73), so the third decimal is uncertain and the second decimal is borderline. The default n_reps=200 is a practical trade-off.

### n_boot choice (with high n_reps)

Bootstrap quantile noise scales as 1/√n_boot and is negligible compared with inter-rep variability (σ_inter ≈ 0.11) when n_reps is high. With n_reps ≥ 1600:

- **n_boot=200–400** suffices: bootstrap variance adds under 0.5% to total; 2.5th percentile estimated from 5–10 order statistics.
- **n_boot=500** is comfortable; going higher gives no meaningful improvement.
- **n_boot=1000** (default in `config.py`) is more than needed for high n_reps; use 200–500 to save time.

When n_reps=200, **n_boot=500** is sufficient; the ~0.001–0.002 difference from n_boot=1000 is swamped by the ~0.007 SE from inter-rep variability.

### Verifying n_boot

To check whether your chosen n_boot is sufficient, run the same scenario with n_boot=500 and n_boot=2000 (same seed). Compare the printed bootstrap CI endpoints:

**Quick check** (n_reps=50, ~1–2 min total): If the difference in CI endpoints is under ~0.003, n_boot=500 is fine for 2-decimal precision. Example:

```bash
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type heavy_center --ci-only --n-reps 50 --n-boot 500 --seed 42 --skip-copula --skip-linear
python run_single_scenario.py --case 3 --n-distinct 4 --dist-type heavy_center --ci-only --n-reps 50 --n-boot 2000 --seed 42 --skip-copula --skip-linear
```

**Thorough check** (n_reps=200, ~5–10 min total): Use n_reps=200 for a more stable comparison and to confirm the averaged CI is well converged.

## Performance and Runtime Estimates

### Optimisation summary

**Vectorization and parallelization:** The original implementation used a scalar loop over bootstrap replicates and power simulations. Several optimisations combine to speed things up:

- **Vectorized Spearman** — argsort-based ranking, O(n log n) per row, replaces per-row scipy calls.
- **Scenario-level parallelization** — **joblib** (`--n-jobs`) farms scenarios out across cores; e.g. with `n_jobs=4` for 4 logical cores, full grid CI goes from ~16 min sequential to ~8 min (**~2×**), and single-scenario CI (200 reps × 1000 boot) from ~42s to ~12s (**3.5×**). Combined with vectorized Spearman, the full CI grid is ~7× faster than the original sequential code.
- **Vectorized data generation** — `config.VECTORIZE_DATA_GENERATION` (default True): power and CI generate all n_sims or n_reps datasets in one batch via `generate_*_batch` functions instead of looping per dataset.
- **Batch bootstrap** — `config.BATCH_CI_BOOTSTRAP` (default True): CI generates all n_reps datasets and n_boot resamples in one batch, then computes Spearman across all bootstrap samples in a single vectorized call. Requires `VECTORIZE_DATA_GENERATION=True`. Can run into memory issues with large runs (e.g. high n_reps × n_boot).

These flags live in `config.py`: `VECTORIZE_DATA_GENERATION`, `BATCH_CI_BOOTSTRAP`, and `CALIBRATION_MODE` (multipoint vs single).



**Calibration:** Uses a rho-independent attenuation ratio cached per (n, k, dist_type). The calibration cost is paid **once per tie structure**, not once per rho value tested during bisection -- eliminating the dominant bottleneck of the original implementation (~60× speedup for power estimation).

**Other optimisations:** `_fit_lognormal` results are LRU-cached; x-value templates are cached and only shuffled per call; a fast inline Spearman (Pearson-of-ranks + t-distribution p-value) replaces the full `scipy.stats.spearmanr` in the power estimation loop.

### Typical runtimes (nonparametric generator, 4-core Windows machine)

| Task | Approximate time |
|------|-----------------|
| Single scenario CI (200 reps x 1000 boot) | ~12s |
| Single scenario power (500 sims, n_cal=300) | ~5s (includes calibration) |
| Single scenario power (10,000 sims, n_cal=300) | ~45s |
| Full grid CI (88 scenarios), n_jobs=1, 200 reps x 1000 boot | ~16 min |
| Full grid CI (88 scenarios), n_jobs=4, 200 reps x 1000 boot | ~8 min |
| Full grid power (88 scenarios, 500 sims, n_cal=300), n_jobs=1 | ~6 min |
| Full grid power (88 scenarios, 500 sims, n_cal=300), n_jobs=4 | ~3 min |

### With Numba JIT (recommended)

Numba JIT compilation adds inner thread parallelism to the ranking and bootstrap loops. Combined with scenario-level `--n-jobs`, expected speedups on a 4-logical-core machine:

| Task | Without Numba | With Numba | Speedup |
|------|--------------|------------|---------|
| Single scenario CI (200 reps x 500 boot) | ~12s | ~3-5s | ~3x |
| Single scenario power (500 sims, n_cal=300) | ~5s | ~2-3s | ~2x |
| Full grid CI (88 scenarios, n_reps=7400, n_boot=500, n_jobs=4) | ~2.5 h | ~20-45 min | ~4-8x |
| Full grid power (88 scenarios, n_sims=10k, n_cal=300, n_jobs=4) | ~30-60 min | ~5-12 min | ~4-6x |

On a 16-vCPU cloud machine (e.g. Hetzner CPX51), full CI grid per generator (88 scenarios, n_reps=7400, n_boot=500, n_jobs=-1) completes in under 12 minutes with Numba and pre-warmed cache.

Copula and linear generators have similar per-sim cost but no calibration overhead. The asymptotic method is instantaneous.

### Tips

- **Benchmarking:** Run sequential and parallel benchmarks separately, one at a time. Concurrent runs cause CPU contention and invalidate timing results.
- Use `--n-sims 500` for exploratory runs (seconds to minutes) vs `10000` for production.
- Use `python run_simulation.py --n-jobs 4` to parallelize across 4 cores and (if this is actually 2 cores with hyperthreading) roughly halve full-grid runtimes. Use `--n-jobs -1` to use all available cores.
- The calibration step adds ~3s (single-point) or ~9s (multipoint, default) per unique (N, k, distribution) tie structure on first run; cached thereafter and reused across all rho values. Use `--calibration-mode single` for faster exploratory runs.
- Bootstrap CIs dominate total runtime when using many reps and resamples. With `--n-reps 200`, use `--n-boot 500` (bootstrap noise is negligible). With `--n-reps 1600` or higher, `--n-boot 200`–`400` suffices; `--n-boot 500` is comfortable. Use `--n-reps 20 --n-boot 500` for quick checks.
- Use `run_single_scenario.py` to test individual scenarios quickly before committing to a full run.
- Filter scenarios with `--cases`, `--n-distinct`, `--dist-types` to reduce the grid.
- Use `--skip-copula --skip-linear` to run only the recommended nonparametric + asymptotic methods.

## Benchmark scripts

Four scripts measure performance of the main optimisations. Run them one at a time (concurrent runs cause CPU contention and invalidate timings; see Tips above).

| Script | Purpose |
|--------|---------|
| `benchmark_batch_vs_no_batch.py` | Single scenario: `batch_bootstrap=False` vs `True` (per-rep vs batch CI path). Case 3, k=4, even, n_reps=200, n_boot=1000. |
| `benchmark_full_grid.py` | Full 88-scenario CI grid: old path vs new path, sequential vs parallel, and Numba thread variants. Uses n_reps=200, n_boot=1000. |
| `benchmark_ci_bootstrap.py` | CI bootstrap with batch path: multipoint vs single-point calibration. Single scenario and 22-scenario subset. `--quick` runs only single-scenario (~1 min). |
| `benchmark_power.py` | Power simulation: vectorized vs scalar data generation, multipoint vs single calibration, sequential vs parallel full grid. `--quick` skips full grid. |

```bash
# Quick single-scenario comparisons
python benchmark_batch_vs_no_batch.py
python benchmark_ci_bootstrap.py --quick
python benchmark_power.py --quick

# Full benchmarks (several minutes each)
python benchmark_full_grid.py
python benchmark_ci_bootstrap.py
python benchmark_power.py
```

See `docs/BENCHMARKING_FINDINGS.md` for results and notes on nested parallelism and memory usage.

## Cloud Deployment

### Running on a cloud VM (e.g. Hetzner CPX51, 16 vCPU)

**With pre-copied Numba cache (recommended):**

```bash
# On local machine (after running warm_up_numba.py):
rsync -avz ~/.cache/numba/ root@YOUR-IP:~/.cache/numba/

# On cloud VM:
python run_simulation.py --n-sims 10000 --skip-copula --skip-linear --n-jobs -1 --seed 42
```

**Full CI grid (all generators):**

```bash
python -c '
import time, pickle
from confidence_interval_calculator import run_all_ci_scenarios
for gen in ["nonparametric", "copula", "linear"]:
    t0 = time.time()
    print(f"Starting {gen}...")
    res = run_all_ci_scenarios(generator=gen, n_reps=7400, n_boot=500, n_jobs=-1, seed=42)
    with open(f"ci_{gen}.pkl", "wb") as f: pickle.dump(res, f)
    print(f"{gen} done in {time.time()-t0:.1f} s")
'
```

## References

- Karwowski MP, Stamoulis C, Wenren LM, Faboyede GM, Quinn N, Grodin MA, Bellinger DC, Woolf AD. Blood and Hair Aluminum Levels, Vaccine History, and Early Infant Development: A Cross-Sectional Study. *Acad Pediatr*. 2018 Mar;18(2):161--165. [doi:10.1016/j.acap.2017.09.003](https://doi.org/10.1016/j.acap.2017.09.003)
- Bonett DG, Wright TA (2000). Sample size requirements for Pearson, Kendall, and Spearman correlations. *Psychometrika*, 65(1), 23--28. [doi:10.1007/BF02294183](https://doi.org/10.1007/BF02294183)
- Conover WJ, Iman RL (1981). Rank transformations as a bridge between parametric and nonparametric statistics. *The American Statistician*, 35(3), 124--129.
- Fieller EC, Hartley HO, Pearson ES (1957). Tests for rank correlation coefficients. I. *Biometrika*, 44(3--4), 470--481. [doi:10.1093/biomet/44.3-4.470](https://doi.org/10.1093/biomet/44.3-4.470)
- Iman RL, Conover WJ (1982). A distribution-free approach to inducing rank correlation among input variables. *Communications in Statistics - Simulation and Computation*, 11(3), 311--334.
- Wikipedia: [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) (Monte Carlo simulation section)
- Wikipedia: [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) (Fisher transformation, references)
